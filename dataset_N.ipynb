{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69e387a7-400e-42e5-a35c-966b70ea3208",
   "metadata": {},
   "source": [
    "# DATA_SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a57c49a4-5db1-4b56-b280-ba6879d7ebc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3135dc85-cc42-4300-ac13-0c8f91f59208",
   "metadata": {},
   "source": [
    "## Creamos la clase de datos \"DATA_SET\"\n",
    "\n",
    "En la siguiente celda se crea la clase \"Data_set\" la cual se encargara de obtener datos esenciales de cada uno de nuestros audios dentro de nuestro conjunto de datos. Como lo es la dirreción de la pista de audio, el \"device\" o dispositivo encargado de procesar el conjunto de datos y por último los datos de \"sample rate\" y \"number of samples\" de cada pista.\n",
    "\n",
    "De igual forma existen funciones que ayudan a los datos de nuestra Data_set a mantener un estandar, así evitando problemas futuros a la hora de utilizar diferentes datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f616602a-bbdd-4e69-91f5-bbbd0c2cf2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_set(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 annotations_file,\n",
    "                 audio_dir,\n",
    "                 target_sample_rate,\n",
    "                 num_samples,\n",
    "                 device):\n",
    "        self.annotations = pd.read_csv(annotations_file)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.device = device\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_sample_path = self._get_audio_sample_path(index)\n",
    "        label = self._get_audio_sample_label(index)\n",
    "        signal, sr = torchaudio.load(audio_sample_path)\n",
    "        signal = signal.to(self.device)\n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "        return signal, label \n",
    "\n",
    "    def _cut_if_necessary(self, signal):\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:, :self.num_samples]\n",
    "        return signal\n",
    "\n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        length_signal = signal.shape[1]\n",
    "        if length_signal < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - length_signal\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "\n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
    "            resampler = resampler.to(self.device)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "\n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal\n",
    "\n",
    "    def _get_audio_sample_path(self, index):\n",
    "        fold = f\"fold{self.annotations.iloc[index, 5]}\"\n",
    "        path = os.path.join(self.audio_dir, fold, self.annotations.iloc[\n",
    "            index, 0])\n",
    "        return path\n",
    "\n",
    "    def _get_audio_sample_label(self, index):\n",
    "        return self.annotations.iloc[index, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3f21a8-27b8-42f6-a149-4c168dff7f67",
   "metadata": {},
   "source": [
    "## Asignación de variables para el conjunto de datos.\n",
    "\n",
    "En la siguiente celda se asignan las variables necesarias para leer nuestro conjunto de datos.\n",
    "\n",
    "- ANNOTATIONS_FILE\n",
    "    \n",
    "    Es la dirección en el equipo de los datos que pose nuestro conjunto de datos.\n",
    "\n",
    "- AUDIO_DIR\n",
    "    \n",
    "    Es la dirreción de la carpeta que contiene todas las muestras de audio de nuestro conjunto de datos.\n",
    "\n",
    "- SAMPLE_RATE\n",
    "    \n",
    "    Definimos el numero de muestras recuperadas en un segundo de audio de cada una de nuestras pistas, en otras palabras, las fragmentos que tendra cada segundo de audio.\n",
    "    \n",
    "- NUM_SAMPLES\n",
    "    \n",
    "    Al igual que \"SAMPLE_RATE\" es el número de muestras obtenidas en un segundo, pero normalmente se utiliza esta variable para los procesos de lectura o escritura.\n",
    "    \n",
    "- device \n",
    "    \n",
    "    Esta variable asigna el dispositivo que llevará acabo todo el procesamiento de datos. En nuestro caso si \"\"cuda\" o el procesamiento con base en la GPU está disponible, se le asignará dicha variable. En caso contrario, la variable será la default o lo que es lo mismo, el \"cpu\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0ecc1c9-f1af-4462-bb75-ee5c842316de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "ANNOTATIONS_FILE = \"./UrbanSound8K/metadata/UrbanSound8K.csv\"\n",
    "AUDIO_DIR = \./UrbanSound8K/audio\"\n",
    "SAMPLE_RATE = 44100\n",
    "NUM_SAMPLES = 44100\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df84b910-ce19-4da1-9e08-a3221fb7c95d",
   "metadata": {},
   "source": [
    "## Mel_spectrogram\n",
    "\n",
    "En la siguiente celda creamos el especograma de Mel o \"Mel_spectrogram\" con las siguientes variables:\n",
    "\n",
    "- n_fft\n",
    "\n",
    "    Esta variable indica el tamaño de la \"ventana\" que pasará por la señal. Este dato está relacionado con la \"Fast Fourier Transformation\".\n",
    "    \n",
    "    \n",
    "- hop_length\n",
    "\n",
    "    Esta variable indica el salto que dará cada que termine con una de las secciones, dicho salta puede ser mayor al de la ventana, perdiento posibles datos; o puede ser menor, retomando muestras antes tomadas.\n",
    "    \n",
    "    \n",
    "- n_mels\n",
    "\n",
    "    Dado que trabajamos con el espectrograma de Mel, debemos contar con la variable de números de mels, es decir, cuantas \"tonos\" (melódicas) vamos a identificar.\n",
    "    \n",
    "Finalmente creamos nuestra variable \"dts\" a través de nuestra clase \"Data_set\" y confirmamos que todos los datos de nuestro conjunto de datos se han asignado. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc86065a-2e1c-4924-99cc-decff74c7dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8732 samples in the dataset.\n"
     ]
    }
   ],
   "source": [
    "n_fft = 1024\n",
    "hop_length = 256 # Salto de un 25%\n",
    "n_mels = 115      # Notas en el piano DOCUMENTACIÓN\n",
    "\n",
    "\n",
    "mel_spectrogram = torchaudio.transforms.MelSpectrogram(sample_rate=SAMPLE_RATE,\n",
    "                                                       n_fft=n_fft, hop_length=hop_length,\n",
    "                                                       n_mels=n_mels)\n",
    "\n",
    "dts = Data_set(ANNOTATIONS_FILE,\n",
    "                   AUDIO_DIR,\n",
    "                   mel_spectrogram,\n",
    "                   SAMPLE_RATE,\n",
    "                   NUM_SAMPLES,\n",
    "                   device)\n",
    "print(f\"There are {len(dts)} samples in the dataset.\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0b4531-800f-4cea-896a-324c789f374a",
   "metadata": {},
   "source": [
    "## Resultado de la clase Data_set\n",
    "\n",
    "Después de someter todas nuestras variables y datos a nuestra clase Data_set. Obtenemos dos datos por cada elemento dentro de nuestra dts:\n",
    "\n",
    "- signal\n",
    "\n",
    "    Esta variable refleja la señal  en modo de \"tensor\" y el \"device\" ocupado en el proceso. Un \"tensor\" es una representación algebraica que descibre la relación multilineal entre conjunto de datos, también puede ser visto como matrices multidimensionales.\n",
    "    \n",
    "- classes\n",
    "\n",
    "    Cada una de las muestras pertenece a una clase dentro de nuestro conjunto de datos. En nuestro caso cada una de las clases esta reflejada por un numero que esta asociado a un nombre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2d47025-717b-48e3-b718-57c9c82632ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0040,  0.0097,  0.0151,  ..., -0.0247, -0.0329, -0.0408]],\n",
       "        device='cuda:0'),\n",
       " 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal, classes = dts[795]\n",
    "signal, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b150887-f4c9-4145-9a4f-bc761d5ad3a4",
   "metadata": {},
   "source": [
    "## Resultado de nuestras señales dentro de nuestro conjunto de datos.\n",
    "\n",
    "Finalmente nuestra señal esta dividida en 3 dimensiones. torch.Size([x, y, z])\n",
    "\n",
    "- Dimensión x\n",
    "\n",
    "    Esta dimensión refleja el número de muestras que tiene nuestra señal, en este caso por que se trata de una sola imagen o no un conjunto de imagenes RGB, la dimensión de la señal es 1.\n",
    "    \n",
    "\n",
    "- Dimensión y\n",
    "\n",
    "    Cada imagen esta dividida en un numero de bloques asignados por la variable \"n_Mels\", es decir que esta dividida por el número de notas a identificar.\n",
    "    \n",
    "    \n",
    "- Dimensión z\n",
    "\n",
    "    Esta dimensión es asignada por el número de ventanas totales que se pudieron obtener gracias a la variable \"hop_length\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "838c9889-6aa0-478b-8fb9-d823e68358c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 44100]), 2, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal.size(), signal.ndim, classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
